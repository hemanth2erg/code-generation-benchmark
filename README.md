Benchmarking GPT-4, Claude, and Mistral on code generation tasks using correctness, execution, and rubric-based scoring.

## Features
- 100+ prompts across Python, C++, and Java
- Execution checker with timeout control
- Summary report in CSV & Markdown

## Tech Stack
Python, Pytest, HuggingFace, OpenAI API

## Usage
```bash
python benchmark.py --model=gpt-4
```

## Author
Hemanth Matta
